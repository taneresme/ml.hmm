{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM)\n",
    "HMM is a probabilistic sequence model. \n",
    "A sequence model or sequence classifier is a model whose job is to assign a label or class to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels. \n",
    "HMM computes a probability distribution over possible sequences of labels and choose the best label sequence. \n",
    "HMM is a key model for language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "A probabilistic time-series model requires a specification of the joint distribution $p(v_1, \\dots ,v_T)$. \n",
    "We cannot expect to independently specify all the exponentially many entries and are focused to make the simplified models.\n",
    "\n",
    "> Note : Time series notation $x_{a:b} \\equiv x_a, x_{a+1}, \\dots ,x_{b}$ with $ x_{a:b} = x_a$ for $b \\leq a$\n",
    "\n",
    "For time-series data $v_1, \\dots ,v_T$ we need a model $p(v_{1:T})$.\n",
    "\n",
    "$$\n",
    "p(v_{1:T}) = \\prod_{t=1}^T p(v_t \\mid v_{1:t - 1})\n",
    "$$\n",
    "\n",
    "$p(v_t \\mid v_{1:t - 1}) = p(v_1)$ for $t = 1$. \n",
    "In markov models, assuming that the influence of immediate past is more relevant than the remote past so limited previous observations are required (named first-order markov chain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "HMM allows us to talk about observed events and hidden events.\n",
    "First order hidden markov model with hidden variables (set of states) $h_t \\in \\{1, \\dots ,H\\}, t = 1:T$. \n",
    "The observed (visible) variables (sequence of observations) $v_t \\in \\{1, \\dots ,V\\}$.\n",
    "\n",
    "The crucial difference between markov models and hidden markov models is that in markov model we know what the sequence of states was, but in hidden markov model we can have more than one state that produces same output. Because of that we do not know just by looking at the output what the actual state transitions were and the key thing that we try to do with hidden markov model is by looking at the output, try to deduce what the actual chain was that generated that output.\n",
    "\n",
    "<img src=\"hmm.png\"  style=\"height:20%;width:30%\" />\n",
    "\n",
    "HMM defines a Markov chain on hidden variables $h_{1:T}$. \n",
    "The observed (or ‘visible’) variables are dependent on the hidden variables through an emission $p(v_t \\mid h_t)$. This defines a joint distribution:\n",
    "\n",
    "$$\n",
    "p(h_{1:T}, v_{1:T}) = p(v_1 \\mid h_1) \\cdot p(h_1) \\prod_{t=2}^T p(v_t \\mid h_t) \\cdot p(h_t \\mid h_{t-1})\n",
    "$$\n",
    "\n",
    "For a stationary HMM the transition $p(h_t \\mid h_{t−1})$ and emission $p(v_t \\mid h_t)$ distributions are constant through time.\n",
    "\n",
    "Transition distribution: $A_{i^* i} = p(h_{t+1} = i^* \\mid h_t = i)$. \n",
    "Transition distribution defines a H x H matrix (also called transition probability matrix).\n",
    "Each $A_{i^* i}$ represents the probability of moving from state $i^*$ to state $i$.\n",
    "All transition probabilities for every single state is equal to 1, $\\sum_{i=1}^n A_{i^*i} = 1$.\n",
    "\n",
    "The initial distribution $a_i = p(h_1 = i)$ represents the probability of Markov chain will start at state $i$.\n",
    "This distribution could also be shown as $\\pi = \\pi_1, \\dots ,\\pi_n$ and besides $\\sum_{i=1}^n \\pi_{i} = 1$\n",
    "\n",
    "The sequence of observation likelihoods is named as emission probabilities.\n",
    "Emission distribution: $B_{ij} = p(v_t = i \\mid h_t = j)$. With discrete values $v_t \\in \\{1, \\dots ,V\\}$, emission distribution defines a V x H matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_xtp1_x1:\n",
      "[[ 0.1  0.   0.9]\n",
      " [ 0.9  0.1  0. ]\n",
      " [ 0.   0.9  0.1]]\n",
      "p_y_x:\n",
      "[[ 0.1  0.1  0.9]\n",
      " [ 0.9  0.9  0.1]]\n",
      "p_y_x[y_1,:] : [ 0.1  0.1  0.9]\n",
      "p_x1_y1: [ 0.05  0.05  0.  ]\n",
      "p_x1_y1: [ 0.5  0.5  0. ]\n",
      "p_x2_y1: [ 0.05  0.5   0.45]\n",
      "p_x2_y12: [ 0.01086957  0.10869565  0.88043478]\n",
      "p_y2_y1: [ 0.46  0.54]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_x1 = np.array([0.5, 0.5, 0])\n",
    "\n",
    "p_xtp1_x1 = np.array([[0.1, 0, 0.9], [0.9, 0.1, 0], [0, 0.9, 0.1]])\n",
    "\n",
    "print(\"p_xtp1_x1:\")\n",
    "print(p_xtp1_x1)\n",
    "\n",
    "p_y_x = np.array([[0.1, 0.1, 0.9], [0.9, 0.9, 0.1]])\n",
    "print(\"p_y_x:\")\n",
    "print(p_y_x)\n",
    "\n",
    "y_1 = 0\n",
    "\n",
    "print(\"p_y_x[y_1,:] :\", p_y_x[y_1,:] )\n",
    "\n",
    "p_x1_y1 = p_y_x[y_1,:] * p_x1\n",
    "print(\"p_x1_y1:\", p_x1_y1)\n",
    "p_x1_y1 = p_x1_y1 / np.sum(p_x1_y1)\n",
    "print(\"p_x1_y1:\", p_x1_y1)\n",
    "\n",
    "p_x2_y1 = p_xtp1_x1.dot(p_x1_y1)\n",
    "print(\"p_x2_y1:\", p_x2_y1)\n",
    "\n",
    "y_2 = 0\n",
    "\n",
    "p_x2_y12 = p_y_x[y_2, :] * p_x2_y1\n",
    "p_x2_y12 = p_x2_y12 / np.sum(p_x2_y12)\n",
    "print(\"p_x2_y12:\", p_x2_y12)\n",
    "\n",
    "p_y2_y1 = p_y_x.dot(p_x2_y1)\n",
    "print(\"p_y2_y1:\", p_y2_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [stanford](https://web.stanford.edu/~jurafsky/slp3/9.pdf), Speech and Language Processing\n",
    "* Bayesian reasoning and machine learning, David Barber"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
